{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jay\\anaconda3\\envs\\myenv\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\Jay\\anaconda3\\envs\\myenv\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\Jay\\anaconda3\\envs\\myenv\\lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'<br />', ' ', text) \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  \n",
    "    text = nltk.word_tokenize(text)  \n",
    "    return ' '.join(text)\n",
    "\n",
    "# Apply preprocessing to the review column\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# PyTorch tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df['cleaned_review']), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def encode(text):\n",
    "    return [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "df['encoded_review'] = df['cleaned_review'].apply(encode)\n",
    "\n",
    "def pad_sequences_torch(sequences, max_len):\n",
    "    return pad_sequence([torch.tensor(seq[:max_len]) for seq in sequences], \n",
    "                        batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "\n",
    "max_seq_length = 100\n",
    "padded_sequences = pad_sequences_torch(df['encoded_review'], max_seq_length)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(glove_file_path, vocab, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coeffs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coeffs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for i, word in enumerate(vocab.get_itos()):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "glove_file_path = 'glove.6B.100d.txt'  \n",
    "embedding_dim = 100\n",
    "embedding_matrix = load_glove_embeddings(glove_file_path, vocab, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix=None):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "\n",
    "model_rnn_glove = VanillaRNN(vocab_size=len(vocab), embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=output_dim, embedding_matrix=embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix=None):\n",
    "        super(LSTM, self).__init__()\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "model_lstm_glove = LSTM(vocab_size=len(vocab), embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=output_dim, embedding_matrix=embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6912204095840454\n",
      "Epoch 2, Loss: 0.6920831412315369\n",
      "Epoch 3, Loss: 0.6879197552680969\n",
      "Epoch 4, Loss: 0.6880878195762634\n",
      "Epoch 5, Loss: 0.6868881858825684\n",
      "Accuracy: 0.5249000191688538\n",
      "Epoch 1, Loss: 0.6874509769439697\n",
      "Epoch 2, Loss: 0.5404983124256134\n",
      "Epoch 3, Loss: 0.4369895175457001\n",
      "Epoch 4, Loss: 0.41177299323081973\n",
      "Epoch 5, Loss: 0.3931473733663559\n",
      "Accuracy: 0.8169000148773193\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "def train_model_with_loader(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch).squeeze(1)\n",
    "            loss = criterion(predictions, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}')\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test).squeeze(1)\n",
    "        predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "        accuracy = (predicted_labels == y_test).float().mean()\n",
    "        print(f'Accuracy: {accuracy.item()}')\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer_rnn = optim.Adam(model_rnn_glove.parameters())\n",
    "optimizer_lstm = optim.Adam(model_lstm_glove.parameters())\n",
    "\n",
    "\n",
    "train_model_with_loader(model_rnn_glove, train_loader, criterion, optimizer_rnn)\n",
    "evaluate_model(model_rnn_glove, X_test, y_test)\n",
    "\n",
    "train_model_with_loader(model_lstm_glove, train_loader, criterion, optimizer_lstm)\n",
    "evaluate_model(model_lstm_glove, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6945618580818176\n",
      "Epoch 2, Loss: 0.6867848712921143\n",
      "Epoch 3, Loss: 0.6694758591651917\n",
      "Epoch 4, Loss: 0.656980727481842\n",
      "Epoch 5, Loss: 0.6226174460887909\n",
      "Accuracy: 0.5525000095367432\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "model_rnn_learned = VanillaRNN(vocab_size=len(vocab), embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_rnn_learned = optim.Adam(model_rnn_learned.parameters())\n",
    "\n",
    "train_model_with_loader(model_rnn_learned, train_loader, criterion, optimizer_rnn_learned)\n",
    "\n",
    "evaluate_model(model_rnn_learned, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6868738242149353\n",
      "Epoch 2, Loss: 0.6636881715774536\n",
      "Epoch 3, Loss: 0.49238369221687317\n",
      "Epoch 4, Loss: 0.33487696602344513\n",
      "Epoch 5, Loss: 0.2565517388224602\n",
      "Accuracy: 0.8098999857902527\n"
     ]
    }
   ],
   "source": [
    "model_lstm_learned = LSTM(vocab_size=len(vocab), embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_lstm_learned = optim.Adam(model_lstm_learned.parameters())\n",
    "\n",
    "train_model_with_loader(model_lstm_learned, train_loader, criterion, optimizer_lstm_learned)\n",
    "evaluate_model(model_lstm_learned, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
